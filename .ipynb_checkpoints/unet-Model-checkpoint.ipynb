{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from skimage.io import imread_collection\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "widthImg = 128\n",
    "heightImg = 128\n",
    "chanels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/qubvel/segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "from segmentation_models import Unet\n",
    "from segmentation_models import get_preprocessing\n",
    "from segmentation_models.losses import bce_jaccard_loss\n",
    "from segmentation_models.metrics import iou_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras.layers import Input, Conv2D, Reshape\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './'\n",
    "TRAIN = './train_v2/'\n",
    "TEST = './test_v2/'\n",
    "SEGMENTATION = './train_ship_segmentations_v2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = 2   #number of workers for data loader\n",
    "arch = resnet34 #specify target architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = [f for f in os.listdir(TRAIN)]\n",
    "tr_n, val_n = train_test_split(train_names, test_size=0.05, random_state=42)\n",
    "segmentation_df = pd.read_csv(os.path.join(PATH, SEGMENTATION)).set_index('ImageId')\n",
    "# print(segmentation_df.columns)\n",
    "# segmentationNames = segmentation_df['ImageId'].tolist()\n",
    "# # print(segmentationNames)\n",
    "# segmentation_df.set_index(segmentationNames,inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00003e153.jpg\n",
      "42556\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "newSeg = pd.read_csv(os.path.join(PATH, SEGMENTATION))\n",
    "print(newSeg.iloc[0]['ImageId'])\n",
    "segDict = dict()\n",
    "# print(newSeg.head)\n",
    "for i in range(len(newSeg)):\n",
    "    temp = newSeg.iloc[i]['EncodedPixels']\n",
    "    # print(temp)\n",
    "    if(type(temp)==str):\n",
    "        segDict[newSeg.iloc[i]['ImageId']] = newSeg.iloc[i]['EncodedPixels']\n",
    "print(len(segDict))\n",
    "# print(newSeg.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40428\n"
     ]
    }
   ],
   "source": [
    "def cut_empty(names):\n",
    "    return [name for name in names \n",
    "            if(type(segmentation_df.loc[name]['EncodedPixels']) != float)]\n",
    "\n",
    "tr_n = cut_empty(tr_n)\n",
    "val_n = cut_empty(val_n)\n",
    "print(len(tr_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PILImage mode=I size=768x768\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEeCAYAAABcyXrWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFPklEQVR4nO3bPYtcZRiA4XdmNiYmm8QiihgFwUaJYGMsbCWClZVW/hz/hmBnIQiWIljYaSolqIUR/GjEZJEkrvnY2bHebCKjbO7ZZK6rfM/h8MAMN+dzslgsBkBpuuoBgPUjPEBOeICc8AA54QFywgPkNv5t44XpO561A//L57sfT+63zRkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxAbmPVA7CmJpMxO3NmjOlk7/p8PuZXt8ZYLFYzFwnhWXeTyZhubu5bnp46Oa6ff3Ys7g7Dfdw6OR1bb94c09l8qf2PHbsz3j/36TgxvbVn/csbL46Lbzw95leuLnUcHk7Cs+7Ovzze/vCLcfyuADwx2x5vHb8+pmO58MwmB3XV/sO4ODt7QMfisBKeNbd7dGO8d+ry2Jweu8fWWT7PGGNMJsvFjoeXm8scKq8evTGuvf78qsfgARMeDpXT08fHzdP+lo86vzCQE551t1iM+fDompbwrLkj3/40Pvjz3KrHYM0Iz5rb3d4eWzsnVj3GHndOeqr1qBMeDp3ZhSurHoEHTHg4dB7bWO7tZx5eXiDkwGzv3h5bu7eX3v+rm8+Mj35/bd/6H5eeGqfG5YMcjUNGeBif/PjK+Ht+ZN/6Zz+/NObfnF76OJu/LMaTX28tvf/k2l9j59ff9q2/MFxqPeqEZ80tdnbGc+9+Py7d45uss4vv/vNX4i6SWIbwMMauXNBycxnICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBOeICc8AA54QFywgPkhAfICQ+QEx4gJzxATniAnPAAOeEBcsID5IQHyAkPkBMeICc8QE54gJzwADnhAXLCA+SEB8gJD5ATHiAnPEBuslgsVj0DsGac8QA54QFywgPkhAfICQ+QEx4g9w/ZbGj+/5FfDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def get_mask(img_id, df):\n",
    "#     shape = (768,768)\n",
    "#     img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "#     masks = df.loc[img_id]['EncodedPixels']\n",
    "#     if(type(masks) == float): return img.reshape(shape)\n",
    "#     if(type(masks) == str): masks = [masks]\n",
    "#     for mask in masks:\n",
    "#         s = mask.split()\n",
    "#         for i in range(len(s)//2):\n",
    "#             start = int(s[2*i]) - 1\n",
    "#             length = int(s[2*i+1])\n",
    "#             img[start:start+length] = 1\n",
    "#     return img.reshape(shape).T\n",
    "# for key in segDict:\n",
    "#     print(segDict[key])\n",
    "#     PILImage.show(segDict[key])\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=PILImage.create(\"./train_v2/{}\".format(tr_n[0]))\n",
    "newSize=(128,128)\n",
    "im1 = im.resize(newSize)\n",
    "# im1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf.keras'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.set_framework('tf.keras')\n",
    "sm.framework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = 'resnet34'\n",
    "preprocess_input = get_preprocessing(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2 as cv\n",
    "# # file=\"0017c19d6.jpg\"\n",
    "# ar=segDict[\"0017c19d6.jpg\"].split(\" \")\n",
    "\n",
    "# rle=[int(x) for x in ar]\n",
    "# pixel,pixel_count = [],[]\n",
    "# [pixel.append(rle[i]) if i%2==0 else pixel_count.append(rle[i]) for i in range(0, len(rle))]\n",
    "# rle_pixels = [list(range(pixel[i],pixel[i]+pixel_count[i])) for i in range(0, len(pixel))]\n",
    "# rle_mask_pixels = sum(rle_pixels,[]) \n",
    "# shape=np.array(cv.imread(\"./train_v2/{}\".format(file))).shape\n",
    "# # plt.imshow(cv.imread(\"./train_v2/{}\".format(file)))\n",
    "# mask_img = np.zeros((shape[0]*shape[1],1), dtype=int)\n",
    "# mask_img[rle_mask_pixels] = 255\n",
    "# l,b=cv.imread(\"./train_v2/{}\".format(file)).shape[0], cv.imread(\"./train_v2/{}\".format(file)).shape[1]\n",
    "# print(l,b)\n",
    "# mask = np.reshape(mask_img, (b, l)).T\n",
    "# msk = PILImage.create(mask)\n",
    "# # plt.imshow(mask)\n",
    "# msk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar=segmentation_df.loc[file]\n",
    "# setMasks = []\n",
    "# for i in range(1000):\n",
    "#     file=tr_n[i]\n",
    "#     # ar=segmentation_df.loc[file][0].split(\" \")\n",
    "#     rle=[int(x) for x in ar]\n",
    "#     pixel,pixel_count = [],[]\n",
    "#     [pixel.append(rle[i]) if i%2==0 else pixel_count.append(rle[i]) for i in range(0, len(rle))]\n",
    "#     rle_pixels = [list(range(pixel[i],pixel[i]+pixel_count[i])) for i in range(0, len(pixel))]\n",
    "#     rle_mask_pixels = sum(rle_pixels,[]) \n",
    "#     shape=np.array(cv.imread(\"./train_v2/{}\".format(file))).shape\n",
    "#     # plt.imshow(cv.imread(\"./train_v2/{}\".format(file)))\n",
    "#     mask_img = np.zeros((shape[0]*shape[1],1), dtype=int)\n",
    "#     mask_img[rle_mask_pixels] = 255\n",
    "#     l,b=cv.imread(\"./train_v2/{}\".format(file)).shape[0], cv.imread(\"./train_v2/{}\".format(file)).shape[1]\n",
    "#     print(l,b)\n",
    "#     mask = np.reshape(mask_img, (b, l)).T\n",
    "#     msk = PILImage.create(mask)\n",
    "#     # plt.imshow(mask)\n",
    "#     # msk.show()\n",
    "#     setMasks.append(msk)\n",
    "# print(len(setMasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "newSegDict = dict()\n",
    "newImgDict = dict()\n",
    "# print(\"./train_v2/{}\".format(\"atul.jpg\"))\n",
    "for key in segDict:\n",
    "    count+=1\n",
    "    im=PILImage.create(\"./train_v2/{}\".format(key))\n",
    "    newSegDict[key] = segDict[key]\n",
    "    newImgDict[key] = im\n",
    "    if(count>1000):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocess_input(newImgDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5\n",
      "85524480/85521592 [==============================] - 115s 1us/step\n",
      "85532672/85521592 [==============================] - 115s 1us/step\n"
     ]
    }
   ],
   "source": [
    "model = Unet(backbone, encoder_weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "newSegDict = []\n",
    "newImgDict = []\n",
    "newValDict = []\n",
    "newValImg = []\n",
    "# print(\"./train_v2/{}\".format(\"atul.jpg\"))\n",
    "for key in segDict:\n",
    "    count+=1\n",
    "    im=PILImage.create(\"./train_v2/{}\".format(key))\n",
    "    newSize=(128,128)\n",
    "    im1 = im.resize(newSize)\n",
    "    # im1.show()\n",
    "    if(count<900):\n",
    "        newSegDict.append(segDict[key])\n",
    "        newImgDict.append(im)\n",
    "    else:\n",
    "        newValDict.append(segDict[key])\n",
    "        newValImg.append(im)\n",
    "    if(count>1000):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newSegDict1 = np.array(newSegDict).astype(np.float32)\n",
    "# newImgDict1 = np.array(newImgDict).astype(np.float32)\n",
    "# newValDict1 = np.array(newValDict).astype(np.float32)\n",
    "# newValImg1 = np.array(newValImg).astype(np.float32)\n",
    "\n",
    "# arrXVal = np.array(np.asarray(newSegDict[0]))\n",
    "print(len(newSegDict))\n",
    "arrXVal = []\n",
    "for i in range(1,len(newSegDict1)-1):\n",
    "    tempImg = np.asarray(newSegDict1[i])\n",
    "    arrXVal.append(tempImg)\n",
    "    # print(\"Hi\")\n",
    "    \n",
    "# arrXTrain = np.array(np.asarray(newImgDict[0]))\n",
    "arrXTrain = []\n",
    "for i in range(1,len(newImgDict1)-1):\n",
    "    tempImg = np.asarray(newImgDict1[i])\n",
    "    arrXTrain.append(tempImg)\n",
    "    \n",
    "# arrYVal = np.array(np.asarray(newValDict1[0]))\n",
    "arrYVal = []\n",
    "for i in range(1,len(newValDict1)-1):\n",
    "    tempImg = np.asarray(newValDict1[i])\n",
    "    arrYVal.append(tempImg)\n",
    "    \n",
    "# arrYTrain = np.array(np.asarray(newValImg1[0]))\n",
    "arrYTrain = []\n",
    "ctemp = 0\n",
    "for i in range(1,len(newValImg1)-1):\n",
    "    tempImg = np.asarray(newValImg1[i])\n",
    "    if(ctemp==0):\n",
    "        print(tempImg.shape)\n",
    "        ctemp+=1\n",
    "    arrYTrain.append(tempImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocess_input(arrXTrain)\n",
    "x_val = preprocess_input(arrYTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arrXTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_9\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(16, 768, 3) dtype=uint8>, <tf.Tensor 'IteratorGetNext:1' shape=(16, 768, 3) dtype=uint8>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [207]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Unet(backbone, input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m3\u001b[39m), encoder_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43marrXVal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrYVal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\manav\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_9\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(16, 768, 3) dtype=uint8>, <tf.Tensor 'IteratorGetNext:1' shape=(16, 768, 3) dtype=uint8>]\n"
     ]
    }
   ],
   "source": [
    "# from fastai import *\n",
    "model = Unet(backbone, input_shape = (768, 768, 3), encoder_weights = 'imagenet')\n",
    "model.compile('Adam', 'binary_crossentropy', ['binary_accuracy'])\n",
    "model.fit(x = x_train, y = x_val, batch_size = 16, epochs = 10, validation_data = (arrXVal, arrYVal,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
